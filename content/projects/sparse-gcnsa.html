<div>
  <h2>Sparse Self-Attention for Graph Convolutional Networks on Low-Homophily Graphs</h2>

  <p>
    See the <a href="assets/projects/sparse-gcnsa-paper.pdf">paper</a> here.
  <p>

  <p>
Graph convolutional networks (GCNs) are the dominant architectures for representation learning on graph-structured data. 
Despite this, modern GCNs face difficulty learning low-homophily graphs, 
an important class of graphs in application. GCNs with self-attention (GCN-SA) are a recent breakthrough offering
exceptional performance on low-homophily graphs: SOTA classification accuracy
on 8 graph datasets of varying homophily, outperforming other competitive models
by a wide margin on low-homophily graphs. Responsible for this improvement
is self-attention (SA), allowing the model to capture long-range dependencies
between nodes. However, GCN-SA is limited by poor runtime scaling of its SA
mechanisms, restricting the model to small graphs. Advancements in the sister
field of graph transformers (GTs) leverage sparse SA mechanisms on graphs (e.g.
EXPHORMER) to address this poor scaling. In this paper we bridge the gap between
advancements in sparse SA with the exceptional performance of GCN-SA on low
homophily graphs. We propose that sparse SA mechanisms replace GCN-SAâ€™s attention 
mechanisms to not only reduce runtime complexity from quadratic to linear,
but to also maintain similar classification accuracy. We perform experiments on 8
graph datasets of varying degrees of homophily with 2 modified GCN-SA-based
models, vanilla GCN-SA, and a GCN baseline.
  </p>

</div>
